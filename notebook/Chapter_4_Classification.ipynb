{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Topics covered in this chapter of the book-\n",
    "\n",
    "* 4.1 An Overview of Classification................. 128\n",
    "* 4.2 Why Not Linear Regression? ................. 129\n",
    "* 4.3 Logistic Regression....................... 130\n",
    "  * 4.3.1 The Logistic Model................... 131 \n",
    "  * 4.3.2 Estimating the Regression Coefficients . . . . . . . . 133 \n",
    "  * 4.3.3 Making Predictions................... 134 \n",
    "  * 4.3.4 Multiple Logistic Regression. . . . . . . . . . . . . . 135 \n",
    "  * 4.3.5 Logistic Regression for >2 Response Classes . . . . . 137\n",
    "* 4.4 Linear Discriminant Analysis ................. 138 \n",
    "  * 4.4.1 Using Bayesâ€™ Theorem for Classification . . . . . . . 138\n",
    "  * 4.4.2 Linear Discriminant Analysis for p=1. . . . . . . . 139 \n",
    "  * 4.4.3 Linear Discriminant Analysis for p >1 . . . . . . . . 142 \n",
    "  * 4.4.4 Quadratic Discriminant Analysis . . . . . . . . . . . 149\n",
    "* 4.5 A Comparison of Classification Methods . . . . . . . . . . . 151\n",
    "* 4.6 Lab: Logistic Regression, LDA, QDA, and KNN . . . . . . 154\n",
    "\n",
    "**Following is the summary of concepts along with data and python code-**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "### Classification Overview \n",
    "\n",
    "All the ways of categorising observations into some defined groups come under classification. So in case of qualitative or categorical response variable Y, the prediction model is also classification model.\n",
    "Three of the most widely-used classifiers, discussed in this chapter: \n",
    "* logistic regression \n",
    "* linear discriminant analysis\n",
    "* K-nearest neighbors\n",
    "We discuss more computer-intensive methods in later chapters, such as generalized additive models (Chapter 7), trees, random forests, and boosting (Chapter 8), and support vector machines (Chapter 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "from patsy import dmatrices\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic regression models the probability that response variable, Y belongs to a particular category. In logistic regression, we use the following logistic function on X in a way that p(X) or Y is between 0 and 1.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Transforming & extending it multiple logistic regression-\n",
    "![image-2.png](attachment:image-2.png)\n",
    "The left-hand side is called the log-odds or logit. We use the maximum likelihood method to estimate Î²0,Î²1,...,Î²p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
       "0     2001  0.381 -0.192 -2.624 -1.055  5.010  1.19130  0.959        Up\n",
       "1     2001  0.959  0.381 -0.192 -2.624 -1.055  1.29650  1.032        Up\n",
       "2     2001  1.032  0.959  0.381 -0.192 -2.624  1.41120 -0.623      Down\n",
       "3     2001 -0.623  1.032  0.959  0.381 -0.192  1.27600  0.614        Up\n",
       "4     2001  0.614 -0.623  1.032  0.959  0.381  1.20570  0.213        Up\n",
       "...    ...    ...    ...    ...    ...    ...      ...    ...       ...\n",
       "1245  2005  0.422  0.252 -0.024 -0.584 -0.285  1.88850  0.043        Up\n",
       "1246  2005  0.043  0.422  0.252 -0.024 -0.584  1.28581 -0.955      Down\n",
       "1247  2005 -0.955  0.043  0.422  0.252 -0.024  1.54047  0.130        Up\n",
       "1248  2005  0.130 -0.955  0.043  0.422  0.252  1.42236 -0.298      Down\n",
       "1249  2005 -0.298  0.130 -0.955  0.043  0.422  1.38254 -0.489      Down\n",
       "\n",
       "[1250 rows x 9 columns]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets using logistic regression to predict percentage returns on the stocks. This data consists of percentage returns for the S&P 500 stock index over 1,250 days, from year 2001 to 2005. \n",
    "# For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. \n",
    "Smarket = pd.read_csv('../data/Smarket.csv', header=0)\n",
    "Smarket.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Intercept   Lag1   Lag2   Lag3   Lag4   Lag5   Volume\n",
      "0           1.0  0.381 -0.192 -2.624 -1.055  5.010  1.19130\n",
      "1           1.0  0.959  0.381 -0.192 -2.624 -1.055  1.29650\n",
      "2           1.0  1.032  0.959  0.381 -0.192 -2.624  1.41120\n",
      "3           1.0 -0.623  1.032  0.959  0.381 -0.192  1.27600\n",
      "4           1.0  0.614 -0.623  1.032  0.959  0.381  1.20570\n",
      "...         ...    ...    ...    ...    ...    ...      ...\n",
      "1245        1.0  0.422  0.252 -0.024 -0.584 -0.285  1.88850\n",
      "1246        1.0  0.043  0.422  0.252 -0.024 -0.584  1.28581\n",
      "1247        1.0 -0.955  0.043  0.422  0.252 -0.024  1.54047\n",
      "1248        1.0  0.130 -0.955  0.043  0.422  0.252  1.42236\n",
      "1249        1.0 -0.298  0.130 -0.955  0.043  0.422  1.38254\n",
      "\n",
      "[1250 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lets set the model equation\n",
    "y, X = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket, return_type = 'dataframe')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>Direction[Up]</td>  <th>  No. Observations:  </th>  <td>  1250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 15 Oct 2020</td> <th>  Pseudo R-squ.:     </th> <td>0.002074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:01:16</td>     <th>  Log-Likelihood:    </th> <td> -863.79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -865.59</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.7319</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.1260</td> <td>    0.241</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.598</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>   -0.0731</td> <td>    0.050</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.171</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0423</td> <td>    0.050</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.140</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag3</th>      <td>    0.0111</td> <td>    0.050</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.087</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag4</th>      <td>    0.0094</td> <td>    0.050</td> <td>    0.187</td> <td> 0.851</td> <td>   -0.089</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag5</th>      <td>    0.0103</td> <td>    0.050</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.087</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Volume</th>    <td>    0.1354</td> <td>    0.158</td> <td>    0.855</td> <td> 0.392</td> <td>   -0.175</td> <td>    0.446</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          Direction[Up]   No. Observations:                 1250\n",
       "Model:                          Logit   Df Residuals:                     1243\n",
       "Method:                           MLE   Df Model:                            6\n",
       "Date:                Thu, 15 Oct 2020   Pseudo R-squ.:                0.002074\n",
       "Time:                        23:01:16   Log-Likelihood:                -863.79\n",
       "converged:                       True   LL-Null:                       -865.59\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.7319\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.1260      0.241     -0.523      0.601      -0.598       0.346\n",
       "Lag1          -0.0731      0.050     -1.457      0.145      -0.171       0.025\n",
       "Lag2          -0.0423      0.050     -0.845      0.398      -0.140       0.056\n",
       "Lag3           0.0111      0.050      0.222      0.824      -0.087       0.109\n",
       "Lag4           0.0094      0.050      0.187      0.851      -0.089       0.107\n",
       "Lag5           0.0103      0.050      0.208      0.835      -0.087       0.107\n",
       "Volume         0.1354      0.158      0.855      0.392      -0.175       0.446\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit logit model \n",
    "logit = sm.Logit(y.iloc[:,1], X).fit()\n",
    "logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50708413, 0.48146788, 0.48113883, ..., 0.5392683 , 0.52611829,\n",
       "       0.51791656])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the probability of the market going up\n",
    "logit.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark it 1 if market goes above 50% else 0\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(1250,1)), columns = ['label'])\n",
    "predict_label.iloc[logit.predict()>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145, 457],\n",
       "       [141, 507]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use confusion matrix between actual and predicted values. \n",
    "# The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. \n",
    "confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5216"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fraction of days for which the prediction was correct\n",
    "np.mean(y.iloc[:,1] == predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this result is misleading because we trained and tested the model on the same set of 1, 250 observations. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data.\n",
    "\n",
    "*To implement this strategy, why don't you take up this next challenge?*\n",
    "\n",
    "Try to first create a vector corresponding to the observations from 2001 through 2004. Then use this vector to create a held out data set of observations from 2005. \n",
    "Do try this challenge and compare on results to the book, page 159-160."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "Earlier we saw, logistic regression involves directly modeling Pr(Y = k|X = x) i.e. conditional distribution of the response Y, given the predictor(s) X. In alternate approaches like LDA, we model the distribution of the predictors X separately in each of the response classes (i.e. given Y ), and then use Bayesâ€™ theorem to flip these around into estimates for Pr(Y = k|X = x).\n",
    "\n",
    "In LDA, we assume that X = (X1,X2,...,Xp) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.\n",
    "The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, with some correlation between each pair of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try on the same market data split into training and testing\n",
    "Smarket_2005 = Smarket.query('Year >= 2005')\n",
    "Smarket_train = Smarket.query('Year < 2005')\n",
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_2005, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shilpa/Library/Python/3.7/lib/python/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(2, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/shilpa/Library/Python/3.7/lib/python/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "skl_lda = LDA(n_components=2).fit(X_train.iloc[:,1:3],y_train.iloc[:,1])\n",
    "X_labels=skl_lda.predict(X_train.iloc[:,1:3])\n",
    "X_prob=skl_lda.predict_proba(X_train.iloc[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_labels=skl_lda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = skl_lda.predict_proba(X_test.iloc[:,1:3]) \n",
    "X_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.iloc[:,1]==X_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n"
     ]
    }
   ],
   "source": [
    "print(skl_lda.means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "### Quadratic Discriminant Analysis\n",
    "\n",
    "Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayesâ€™ theorem in order to per- form prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5992063492063492\n"
     ]
    }
   ],
   "source": [
    "sk_qda=QDA(store_covariance=True).fit(X_train.iloc[:,1:3],y_train.iloc[:,1])\n",
    "X_labels = sk_qda.predict(X_train.iloc[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = sk_qda.predict_proba(X_train.iloc[:,1:3]) #the probability of each sample to belong to each class\n",
    "\n",
    "X_test_labels=sk_qda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = sk_qda.predict_proba(X_test.iloc[:,1:3]) \n",
    "\n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "[array([[ 1.50662277, -0.03924806],\n",
      "       [-0.03924806,  1.53559498]]), array([[ 1.51700576, -0.02787349],\n",
      "       [-0.02787349,  1.49026815]])]\n"
     ]
    }
   ],
   "source": [
    "print(sk_qda.means_)\n",
    "print(sk_qda.covariance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_____________\n",
    "\n",
    "### K-nearest neighbour\n",
    "\n",
    "KNN takes a completely different approach from the classifiers seen in this chapter. In order to make a prediction for an observation X = x, the K training observations that are closest to x are identified. Then X is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary.\n",
    "\n",
    "Therefore, we can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear. On the other hand, KNN does not tell us which predictors are important; we donâ€™t get a table of coefficients.\n",
    "\n",
    "QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible as KNN, QDA can perform better in the presence of a limited number of training observations because it does make some assumptions about the form of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5158730158730159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhb_knn=KNN(n_neighbors=4).fit(X_train.iloc[:,1:3],y_train.iloc[:,1])\n",
    "\n",
    "X_test_labels=nhb_knn.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = nhb_knn.predict_proba(X_test.iloc[:,1:3]) \n",
    "\n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels))\n",
    "nhb_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
