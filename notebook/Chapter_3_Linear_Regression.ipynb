{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Topics covered in this chapter of the book-\n",
    "\n",
    "* 3.1 Simple Linear Regression ................... 61\n",
    "  * 3.1.1 Estimating the Coefficients .............. 61\n",
    "  * 3.1.2 Assessing the Accuracy of the Coefficient Estimates........................ 63\n",
    "  * 3.1.3 Assessing the AccuracyoftheModel . . . . . . . . . 68\n",
    "* 3.2 Multiple Linear Regression .................. 71\n",
    "  * 3.2.1 Estimating the Regression Coefficients . . . . . . . . 72 \n",
    "  * 3.2.2 SomeImportantQuestions .............. 75\n",
    "* 3.3 Other Considerations in the Regression Model . . . . . . . . 82\n",
    "  * 3.3.1 Qualitative Predictors ................. 82\n",
    "  * 3.3.2 Extensions of the Linear Model . . . . . . . . . . . . 86\n",
    "  * 3.3.3 Potential Problems................... 92\n",
    "* 3.4 The Marketing Plan ...................... 102\n",
    "* 3.5 Comparison of Linear Regression with K -Nearest Neighbors............................ 104\n",
    "\n",
    "**Following is the summary of concepts along with data and python code-**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Linear Regression\n",
    "**Linear regression (LR)** is a approach for predicting a quantitative response Y on the basis of some predictor variables, Xs, assumig a linear relationship between Xs and Y. Mathematically, we can write this linear relationship as\n",
    "\n",
    "Y ≈ β0 + β1X1 + β2X2 ... βnxn \n",
    "\n",
    "β0, β1,.. βn are known as the model coefficients or parameters.\n",
    "\n",
    "The ordinary least squares (OLS) approach chooses β0, β1,.. βn to minimize the RSS (residual sum of squares)- the gap between actual Y and predicted Y.\n",
    "\n",
    "Some important questions of linear regression-\n",
    "\n",
    "* *How good is the relationship between the response and predictors?*\n",
    "F-statistic helps us understand which mathematically equates to ((TSS − RSS)/p)/(RSS/(n−p−1)). \n",
    "\n",
    "* *Deciding on important variables, also known as variable selection.*\n",
    "The p-value of the variable is a good indicator but not the only one. Sometimes, if p is large we are likely to make some false discoveries. There are three classical approaches for this task- \n",
    "  * **Forward selection**- Start from null model and keep adding variables to find the lowest RSS.\n",
    "  * **Backward selection**- Start with all variables, and keep removing the variables with larger p-value till to find lowest RSS or get low individual p-value.\n",
    "  * **Mixed selection**- Mix of two. Start with null model, keep adding till p-value of variables gets larger and then remove that variable. Continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model. \n",
    "\n",
    "* *Model fit.*\n",
    "Two of the most common numerical measures of model fit are the RSE and R2, the fraction of variance explained. R2 value close to 1 indicates that the model explains a large portion of the variance in the response variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple regression with 2 predicators- understand quality of model & solution using model statistics like p-value, R-sq etc.\n",
    "boston = pd.read_csv('../data/Boston.csv', header=0)\n",
    "boston.shape\n",
    "lm = smf.ols('medv~lstat+age', data=boston).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple regression with all predicators in data- understand quality of model & solution using model statistics like p-value, R-sq etc.\n",
    "formula = \"medv~\" + \"+\".join(boston.columns.drop([\"medv\"]))\n",
    "lm = smf.ols(formula, data=boston).fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "### Qualitative Predictors\n",
    "\n",
    "These are predictors with with two or more fixed categories of values. In case of two levels, they are also known as a factor or dummy variable. For example- gender, ethinicity etc. \n",
    "\n",
    "With such variables, each of their value (minus 1) is considered as separate predicator. For eg- If our ethinicity variable has 3 categories- Asian, Caucasian, and African American, then model with only ethinicity variable would be-\n",
    "\n",
    "yi = β0 + β1xi1 + β2xi2 + εi where \n",
    "* xi1 = 1 for Asian else 0 \n",
    "* xi2 = 1 for Caucasian else 0\n",
    "* xi1 = 0 & xi2 = 0 condition is for African American\n",
    "\n",
    "making yi =\n",
    "* β0+β1+εi if ith person is Asian\n",
    "* β0+β2+εi if ith person is Caucasian\n",
    "* β0+εi if ith person is African American\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with Qualitative Predictors in following data \n",
    "carseats = pd.read_csv('../data/Carseats.csv', header=0)\n",
    "carseats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_carseats_dummy = smf.ols('Sales ~ Income + Advertising + Price + Age + C(ShelveLoc)', \n",
    "                            data = carseats).fit()\n",
    "lm_carseats_dummy.summary()\n",
    "# see you will get 2 coefficients for ShelveLoc variables as it has 3 values and we are considering them categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______\n",
    "\n",
    "### Non-linear Transformations of the Predictors \n",
    "Two of the most important assumptions in LR that the relationship between the predictors and response are additive and linear. But we can extend linear regression by-\n",
    "* Removing the Additive Assumption\n",
    "\n",
    "Y = β0 +(β1 +β3X2)X1 +β2X2 +ε\n",
    "\n",
    "* Non-linear Relationships\n",
    "\n",
    "Accommodating non-linear relationships through polynomial or logarithmic regression.\n",
    "\n",
    "Y = β0 + β1X1 + β2X1^2 + ε\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets try an non-additive model\n",
    "lm = smf.ols('medv~lstat * age', data=boston).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets try non-linear relationship and compare with linear model\n",
    "lm_order1 = smf.ols('medv~ lstat', data=boston).fit()\n",
    "lm_order2 = smf.ols('medv~ lstat+ I(lstat ** 2.0)', data=boston).fit()\n",
    "print(lm_order2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. \n",
    "We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Comparison of linear n non-linear model using anova\n",
    "table = sm.stats.anova_lm(lm_order1, lm_order2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat2 is far superior to the model that only contains the predictor lstat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______\n",
    "\n",
    "### Potential Problems\n",
    "\n",
    "When we fit a linear regression model to a particular data set, many problems may occur. Most common among these are the following:\n",
    "\n",
    "**1. Non-linearity of the response-predictor relationships**\n",
    "\n",
    "Residual plots are a useful graphical tool for identifying non-linearity. If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as logX, √X, and X2, in the regression model\n",
    "\n",
    "**2. Correlation of error terms**\n",
    "\n",
    "An important assumption of the linear regression model is that the error terms, ε1,ε2,...,εn, are uncorrelated. If the error terms are correlated, we may have an unwarranted sense of confidence in our model. Plots of residuals terms can help indetify the correlation. Such correlations frequently occur in the context of time series data.\n",
    "\n",
    "**3. Non-constant variance of error terms**\n",
    "\n",
    "Another assumption with using OLS in LR is that the error terms have a constant variance, Var(εi) = σ or homoscedasticity. When faced with this problem, one possible solution is to transform the response Y using a concave function such as log Y or Y. Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.\n",
    "\n",
    "**4. Outliers**\n",
    "\n",
    "An outlier is a point for which yi is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection. Residual plots can be used to identify outliers. If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation.\n",
    "\n",
    "**5. High-leverage points**\n",
    "\n",
    "These are outliers in predictor xi. One can use usual methods of outlier detection like plots, Z-Score, IQR etc and remove or impute them as justified before adding predicator to the model. Specifically for leverage, one can quantify an observation’s leverage using leverage statistic. A large value of this statistic indicates an observation with high leverage. \n",
    "\n",
    "**6. Collinearity**\n",
    "\n",
    "Collinearity refers to the situation in which two or more predictor variables are closely related to one another.\n",
    "A simple way to detect collinearity is to look at the correlation matrix of the predictors. In case of multicollinearity, i.e. collinearity between three or more variables even if no pair of variables has a particularly high correlation, is to compute the variance inflation factor (VIF). \n",
    "When faced with the problem of collinearity, there are two simple solutions. \n",
    "The first is to drop one of the problematic variables from the regression. \n",
    "The second solution is to combine the collinear variables together into a single predictor by either taking average of their standardized versions or using dimension reduction methods like PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
