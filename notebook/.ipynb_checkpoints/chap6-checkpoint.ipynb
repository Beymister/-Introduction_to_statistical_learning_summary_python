{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This notebook contains the code for best subset selection, \n",
    "so this notebook may take longer time to run, for faster run, make\n",
    "max_feature into a smaller number\"\"\"\n",
    "max_feature = 3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = pd.read_csv('/Users/shilpa/Documents/blog/Sharing_ISL_python/data/Hitters.csv', header=0, na_values='NA')\n",
    "\n",
    "print(list(Hitters)) # get the header of this data\n",
    "\n",
    "print(Hitters.shape) # get the dimension of this \n",
    "\n",
    "Hitters.head() # pull a sample of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(pd.isnull(Hitters['Salary']))) # number of NAs in Salary column'\n",
    "print(Hitters['Salary'].isnull().sum())\n",
    "\n",
    "Hitters = Hitters.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "Hitters.shape\n",
    "    \n",
    "print(Hitters['Salary'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Hitters.Salary  # the response variable \n",
    "\n",
    "\"\"\"\n",
    "take care of the features \n",
    "1. change category into dummy variables \n",
    "2. Choose (n-1) dummy variable into the feature set: n is the unique values of each categorical variable.\n",
    "\"\"\"\n",
    "\n",
    "dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "X_prep = Hitters.drop (['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X = pd.concat([X_prep,  dummies[['League_A', 'Division_E', 'NewLeague_A']]], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since in Python there is no well-defined function for best subset selection, \n",
    "we will need to define some functions ourselves.\n",
    "1. Define a function to run on a subset of feature and extract RSS\n",
    "2. Select the best model (models) for a fix number of features\n",
    "\"\"\"\n",
    "def getRSS(y, X, feature_list):\n",
    "    model = sm.OLS(y, X[list(feature_list)]).fit()\n",
    "    RSS = ((model.predict(X[list(feature_list)]) - y) ** 2).sum()\n",
    "    return {'Model':model, \"RSS\":RSS}\n",
    "\n",
    "def bestModel(y, X, K):\n",
    "    results = []\n",
    "    for c in itertools.combinations(X.columns, K):\n",
    "        results.append(getRSS(y, X, c))     \n",
    "    model_all =  pd.DataFrame(results)\n",
    "    \n",
    "    best_model = model_all.loc[model_all[\"RSS\"].idxmin()] ## this could be modified to have the top several models\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "for i in range(1,(max_feature+1)):  # for illustration purpuse, I just run for 1 - max_fearure features \n",
    "    models.loc[i] = bestModel(y, X, i)\n",
    "    \n",
    "print(models.loc[2, 'Model'].summary())\n",
    "print(models)\n",
    "# this summay confirms that the best two variable model contains the variables Hits and CRBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this show an example to plot the RSS of best models with different number of parameters\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(models[\"RSS\"])\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_adj = models.apply(lambda a: \n",
    "                            a[1].rsquared_adj, axis=1)\n",
    "# find the adjust R^2, use dir() to identify all available attributes\n",
    "rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following graph shows the adj R^2 is still increasing, \n",
    "in this case, it is a good idea trying models with more features. \n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(rsquared_adj)\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('Adjust R^2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.2 Forward and Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can use the previous user defined function 'def getRSS(y, X, feature_list)' to add 1 feature at a time (start from 0 feature) for forward stepwise selection\n",
    "or delete 1 feature at a time(start from all the features) for backward stepwise selection. \n",
    "\"\"\"\n",
    "def forward_select(y, X, feature_list):\n",
    "    remaining_predictors = [p for p in X.columns if p not in feature_list]\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(getRSS(y, X, feature_list+[p]))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    return best_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "feature_list = []\n",
    "for i in range(1,len(X.columns)+1):\n",
    "    models2.loc[i] = forward_select(y, X, feature_list)\n",
    "    feature_list = models2.loc[i][\"Model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we can compare the results of best subset selection and the forward selection\"\"\"\n",
    "print('Best max_feature variable from best subset selection on tranining')\n",
    "print(models.loc[max_feature, 'Model'].params)\n",
    "print('\\n---------------------------------------------')\n",
    "print('Best max_feature variable from forward selection on tranining')\n",
    "print(models2.loc[max_feature, 'Model'].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_adj = models2.apply(lambda a: \n",
    "                            a[1].rsquared_adj, axis=1)\n",
    "# find the adjust R^2, use dir() to identify all available attributes\n",
    "print(models2,rsquared_adj)\n",
    "# R^2 increasing & RSS lowering but adj R^2 is falling post 10 p hence, we dont need more than 10 p, others are insignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2.iloc[18,1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_select(y, X, feature_list):\n",
    "    results = []\n",
    "    for combo in itertools.combinations(feature_list, len(feature_list)-1):\n",
    "        results.append(getRSS(y, X, combo))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    return best_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The backward selection starts from all the variables of features\n",
    "\"\"\"\n",
    "models3 = pd.DataFrame(columns=[\"RSS\", \"Model\"], index = range(1,len(X.columns)))\n",
    "feature_list = X.columns\n",
    "\n",
    "while(len(feature_list) > 1):\n",
    "    models3.loc[len(feature_list)-1] = backward_select(y, X, feature_list)\n",
    "    feature_list = models3.loc[len(feature_list)-1][\"Model\"].model.exog_names\n",
    "\n",
    "print(models3.loc[max_feature, \"Model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsquared_adj = models3.apply(lambda a: \n",
    "                            a[1].rsquared_adj, axis=1)\n",
    "# find the adjust R^2, use dir() to identify all available attributes\n",
    "print(models3,rsquared_adj)\n",
    "# R^2 increasing & RSS lowering but adj R^2 is falling post 10 p hence, we dont need more than 10 p, others are insignificant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into traning dataset and validation dateset\n",
    "np.random.seed(seed = 21)\n",
    "train_index = np.random.choice([True, False], size = len(y), replace = True, p = [0.7, 0.3]) \n",
    "# random select ~70% of data into traning sample\n",
    "# the rest of the samples will be in testing set\n",
    "test_index = np.invert(train_index)\n",
    "X_train= X[train_index]\n",
    "y_train = y[train_index]\n",
    "X_test = X[test_index]\n",
    "y_test = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We can recyle the old functions. Modification is needed to compute the RSS for the testing data. \n",
    "So we need to add both train and test into the function input (Implement)\n",
    "-OR-: we can wrap the train and test split step into the function(Not Implemented)\n",
    "\"\"\"\n",
    "def getRSS_validation(y_train, X_train, y_test, X_test,  feature_list):\n",
    "    model = sm.OLS(y_train, X_train[list(feature_list)]).fit()\n",
    "    RSS = ((model.predict(X_test[list(feature_list)]) - y_test) ** 2).sum()\n",
    "    return {'Model':model, \"RSS\":RSS}\n",
    "\n",
    "def bestModel_validation(y_train, X_train, y_test, X_test, K):\n",
    "    results = []\n",
    "    for c in itertools.combinations(X_train.columns, K):\n",
    "        results.append(getRSS_validation(y_train, X_train, y_test, X_test, c))     \n",
    "    model_all =  pd.DataFrame(results)\n",
    "    \n",
    "    best_model = model_all.loc[model_all[\"RSS\"].idxmin()] ## this could be modified to have the top several models\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def forward_select_validation(y_train, X_train, y_test, X_test,  feature_list):\n",
    "    remaining_predictors = [p for p in X_train.columns if p not in feature_list]\n",
    "    results = []\n",
    "    for p in remaining_predictors:\n",
    "        results.append(getRSS_validation(y_train, X_train, y_test, X_test, feature_list+[p]))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    return best_model\n",
    "\n",
    "def backward_select_validation(y_train, X_train, y_test, X_test,  feature_list):\n",
    "    results = []\n",
    "    for combo in itertools.combinations(feature_list, len(feature_list)-1):\n",
    "        results.append(getRSS_validation(y_train, X_train, y_test, X_test,  combo))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "    best_model = models.loc[models['RSS'].idxmin()]\n",
    "    return best_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_validation = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "for i in range(1,(max_feature+1)):  # for illustration purpuse, I just run for 1 - max_fearure features \n",
    "    models_validation.loc[i] = bestModel_validation(y_train, X_train, y_test, X_test, i) \n",
    "    \n",
    "    \n",
    "\"\"\"change the function to  forward_select_validation (.) or backward_select_validation(.) \n",
    "for forward selection or backward selection\"\"\" \n",
    "    \n",
    "models2_forward = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "feature_list = []\n",
    "for i in range(1,len(X.columns)+1):\n",
    "    models2_forward.loc[i] = forward_select_validation(y_train, X_train, y_test, X_test,  feature_list)\n",
    "    feature_list = models2_forward.loc[i][\"Model\"].model.exog_names    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best max_feature variable from best subset selection on training')\n",
    "print(models.loc[max_feature, 'Model'].params)\n",
    "print('\\n---------------------------------------------')\n",
    "print('Best max_feature variable from forward selection on training')\n",
    "print(models2.loc[max_feature, 'Model'].params)\n",
    "print('\\n---------------------------------------------')\n",
    "print('Best max_feature variable from backward selection on training')\n",
    "print(models3.loc[max_feature, 'Model'].params)\n",
    "print('\\n---------------------------------------------')\n",
    "print('Best max_feature variable from best subset selection on training and validation split')\n",
    "print(models_validation.loc[max_feature, 'Model'].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this show an example to plot the RSS of best models with different number of parameters for best subset with validation\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(models_validation[\"RSS\"])\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this show an example to plot the RSS of best models with different number of parameters for forward selection with validation\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(models2_forward[\"RSS\"])\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()\n",
    "\"\"\" From above graph, that 6 variables model gives us the best RSS under forward selection. \n",
    "To learn the final model, it is also recommendated to re-train the model on entire data (train + validation). \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_adj = models2_forward.apply(lambda a: \n",
    "                            a[1].rsquared_adj, axis=1)\n",
    "# find the adjust R^2, use dir() to identify all available attributes\n",
    "print(models3,rsquared_adj)\n",
    "\"\"\" this show an example to plot the RSS of best models with different number of parameters for forward selection with validation\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(rsquared_adj)\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()\n",
    "\"\"\" From above graph, that 6 variables model gives us the best RSS under forward selection. \n",
    "To learn the final model, it is also recommendated to re-train the model on entire data (train + validation). \"\"\"\n",
    "# R^2 increasing & RSS lowering but adj R^2 is falling post 10 p hence, we dont need more than 10 p, others are insignificant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This approach is similar to the previous validation idea. \n",
    "The difference is that we break the entire dataset into K different folds. \n",
    "Each run, the model will be trained on all the data from K-1 folds and tested on the remaining fold.\n",
    "Advantages: \n",
    "1. There will be multiple metrics out of testing results => distribution of testing RSS, etc\n",
    "2. The size of traning dataset is much closer to the size of original dateset. This will remove some biases caused by\n",
    "the size difference.\"\"\"\n",
    "\n",
    "k = 10\n",
    "np.random.seed(seed = 21)\n",
    "train_index = np.random.choice(k, size = len(y), replace = True)  # Randomly assign each observations into folds\n",
    "cv_errors = pd.DataFrame(columns=range(1,k+1), index=range(1,len(X.columns) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cv = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "for j in range(1,k+1):\n",
    "    feature_list = []\n",
    "    for i in range(1,len(X.columns)+1):\n",
    "        models_cv.loc[i] = forward_select_validation(y[train_index!= (j-1)], X[train_index != (j-1)], \n",
    "                                                     y[train_index == (j-1)],X[train_index == (j-1)], \n",
    "                                                     feature_list)\n",
    "        \n",
    "        cv_errors[j][i] = models_cv.loc[i][\"RSS\"]\n",
    "        feature_list = models_cv.loc[i][\"Model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_errors_mean = cv_errors.mean(axis = 1)\n",
    "plt.figure()\n",
    "plt.plot(cv_errors_mean)\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From the above plot, we can see that the model with 5 variables yielded the smallest RSS.\n",
    "We can take a closer look at that model summary. \n",
    "We can also see that the model performance for variables 4 - 12 are similar.\"\"\"\n",
    "print(models_cv.loc[5, \"Model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 Lab 2: Ridge Regression and the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn has the ridge and lasso functionality implemented. So here we import those submodules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = pd.read_csv('/Users/shilpa/Documents/blog/Sharing_ISL_python/data/Hitters.csv', header=0, na_values='NA')\n",
    "Hitters = Hitters.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = Hitters.Salary  # the response variable \n",
    "X_prep = Hitters.drop (['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X = pd.concat([X_prep,  dummies[['League_A', 'Division_E', 'NewLeague_A']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)\n",
    "np.linspace?\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(fit_intercept=True, normalize=True)\n",
    "coeffs = []\n",
    "intercepts = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(X, y)\n",
    "    coeffs.append(ridge.coef_)\n",
    "    intercepts.append(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(coeffs))\n",
    "print(len(coeffs[0]))\n",
    "print(len(intercepts))\n",
    "print(intercepts[0]) # try run print len(intercepts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coeffs)\n",
    "ax.set_xscale('log') # try without this line\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = cross_validate(ridge,X, y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, X_test , y_train, y_test = cross_validate.train_test_split(X, y, test_size=0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(fit_intercept=True, normalize=True, alpha=4)\n",
    "ridge.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred = ridge.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge.coef_, index=X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred))        # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7.1 Principal Components Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = pd.read_csv('/Users/shilpa/Documents/blog/Sharing_ISL_python/data/Hitters.csv', header=0, na_values='NA')\n",
    "Hitters = Hitters.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "y = Hitters.Salary  # the response variable \n",
    "dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\n",
    "X_prep = Hitters.drop (['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X = pd.concat([X_prep,  dummies[['League_A', 'Division_E', 'NewLeague_A']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(scale(X))\n",
    "regr = linear_model.LinearRegression()\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcr(X,y,pc):\n",
    "    ''' Principal Component Regression in Python'''\n",
    "    ''' Step 1: PCA on input data'''\n",
    "\n",
    "    # Define the PCA object\n",
    "    pca = PCA()\n",
    "\n",
    "    # Preprocessing (1): first derivative\n",
    "    X_pca = pca.fit_transform(scale(X))[:,:pc]\n",
    "\n",
    "    ''' Step 2: regression on selected principal components'''\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    \n",
    "    # Fit\n",
    "    regr.fit(X_pca, y)\n",
    "\n",
    "    # Calibration\n",
    "    y_train = regr.predict(X_pca)\n",
    "\n",
    "    # Cross-validation\n",
    "    y_cv = cross_val_predict(regr, X_pca, y, cv=20)\n",
    "\n",
    "    # Calculate scores for training and cross-validation\n",
    "    score_train = r2_score(y, y_train)\n",
    "    score_cv = r2_score(y, y_cv)\n",
    "\n",
    "    # Calculate mean square error for training and cross validation\n",
    "    mse_train = mean_squared_error(y, y_train)\n",
    "    mse_cv = mean_squared_error(y, y_cv)\n",
    "\n",
    "    return(y_cv, score_train, score_cv, mse_train, mse_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = []\n",
    "mse_cv = []\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    results =  pcr(X,y,i)\n",
    "    mse_train.append(results[3])\n",
    "    mse_cv.append(results[4])\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(np.arange(1, 20), mse_cv, '-v', label = 'Validation_MSE')\n",
    "plt.plot(np.arange(1, 20), mse_train, '-v', label = 'Train_MSE')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca2 = PCA()\n",
    "# Scale the data\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "X_reduced_test = pca2.transform(scale(X_test))[:,:6]\n",
    "# Train regression model on training data \n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:6], y_train)\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
