{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Topics covered in this chapter of the book-\n",
    "\n",
    "* 3.1 SimpleLinearRegression ................... 61\n",
    "  * 3.1.1 EstimatingtheCoefficients .............. 61\n",
    "  * 3.1.2 Assessing the Accuracy of the Coefficient Estimates........................ 63\n",
    "  * 3.1.3 AssessingtheAccuracyoftheModel . . . . . . . . . 68\n",
    "* 3.2 MultipleLinearRegression .................. 71\n",
    "  * 3.2.1 Estimating the Regression Coefficients . . . . . . . . 72 3.2.2 SomeImportantQuestions .............. 75\n",
    "* 3.3 Other Considerations in the Regression Model . . . . . . . . 82\n",
    "  * 3.3.1 QualitativePredictors ................. 82\n",
    "  * 3.3.2 ExtensionsoftheLinearModel . . . . . . . . . . . . 86\n",
    "  * 3.3.3 PotentialProblems................... 92\n",
    "* 3.4 TheMarketingPlan ...................... 102\n",
    "* 3.5 Comparison of Linear Regression with K -Nearest Neighbors............................ 104\n",
    "\n",
    "**Following is the summary of concepts along with data and python code-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression** is a approach for predicting a quantitative response Y on the basis of some predictor variables, Xs, assumig a linear relationship between Xs and Y. Mathematically, we can write this linear relationship as\n",
    "\n",
    "Y ≈ β0 + β1X1 + β2X2 ... βnxn \n",
    "\n",
    "β0, β1,.. βn are known as the model coefficients or parameters.\n",
    "\n",
    "The ordinary least squares (OLS) approach chooses β0, β1,.. βn to minimize the RSS (residual sum of squares)- the gap between actual Y and predicted Y.\n",
    "\n",
    "Some important questions of linear regression:\n",
    "\n",
    "* Is There a Relationship Between the Response and Predictors?\n",
    "Residual standard error, R-square, and F-statistic are ways to check this.\n",
    "\n",
    "* Deciding on Important Variables, also known as variable selection.\n",
    "The p-value of the variable is a good indicator but not the only one. Sometimes, if p is large we are likely to make some false discoveries. There are three classical approaches for this task- \n",
    "**Forward selection**- Start from null model and keep adding variables to find the lowest RSS. \n",
    "**Backward selection**- Start with all variables, and keep removing the variables with larger p-value till to find lowest RSS or get low individual p-value.\n",
    "**Mixed selection**- Mix of two. Start with null model, keep adding till p-value of variables gets larger and then remove that variable. Continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model. \n",
    "\n",
    "* Model Fit.\n",
    "Two of the most common numerical measures of model fit are the RSE and R2, the fraction of variance explained. R2 value close to 1 indicates that the model explains a large portion of the variance in the response variable. \n",
    "\n",
    "* Predictions-\n",
    "Other Considerations in the Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
